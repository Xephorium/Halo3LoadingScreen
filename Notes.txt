
//////////////////////////////
//// WebGL Halo Animation ////
//////////////////////////////


/*--- Remaining Tasks ---*/

Core Functionality
1. Texture particles (DONE)
2. Camera movement (DONE)
  > Splines! (DONE)
  > Match FOV (DONE)
3. Cubes
4. Lines
5. Background

Enhancements
- Precompute swerve points
- Free camera movement


/*--- Animation Details ---*/

Timing
  Start Delay: 2,000ms (2s ~ 120 Blender Frames)
  Ring Assembly: 66,000ms (66s ~ 3980 Blender Frames)
  End Drift: 7,000ms
  Loop Length: 75,000ms

Pathing
  Focus: Semicircle of slightly lower radius than the ring itself at the ring's vertical center. Radius decreases
         slightly as ring approaches 1/2 completion. Final point is extended toward the back side to center the camera
         during connection.
  Camera: Semicircle that begins about 1/4 through one side of the ring's construction. Starts lower than ring. Speeds
          up until rough speed match at half way point. Slows to let ring pass and rises over top, drifting down and toward center.



/*--- Image Data Organization ---*/

Initial Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Final Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Data Dynamic
  R: Alpha
  G: Brightness
  B: ---
  A: ---

Data Static
  R: Wait
  G: Seed
  B: Ambient
  A: ---


/*--- Frame Buffer Objects ---*/

1. fbo_pos_initial   // Unchanging
2. fbo_pos_final     // Unchanging
3. fbo_pos           // Updated by prog_position
4. fbo_data_dynamic  // Updated by prog_data
5. fbo_data_static   // Unchanging


/*--- Shader Programs ---*/

1. prog_position    // Updates each particle position
2. prog_data        // Updates each particle's dynamic data
3. prog_particle    // Renders particles
4. prog_display     // Displays render to screen


/*--- Cool & Useful Calculations ---*/

  // Procedural Float Generator [-1, 1]
  // Note: Consistently returns the same pseudo-random float for the same two input values.  
  float generate_float(float value_one, float value_two) {
      float seed_one = 78.0;
      float seed_two = 1349.0;
      float magnitude = (mod(floor(value_one * seed_one + value_two * seed_two), 100.0) / 100.0) * 2.0 - 1.0;
      return magnitude;
  }

  // Quadratic Spline Interpolator
  // Note: Returns a position in 3D space representing a particle's location on
  //       a smooth bezier curve between three points given factor t [0-1]. 
  // Source: https://forum.unity.com/threads/getting-a-point-on-a-bezier-curve-given-distance.382785/ 
  vec3 interpolate_location(vec3 v1, vec3 v2, vec3 v3, float t) {
         float x = (((1.0 - t) * (1.0 - t)) * v1.x) + (2.0 * t * (1.0 - t) * v2.x) + ((t * t) * v3.x);
         float y = (((1.0 - t) * (1.0 - t)) * v1.y) + (2.0 * t * (1.0 - t) * v2.y) + ((t * t) * v3.y);
         float z = (((1.0 - t) * (1.0 - t)) * v1.z) + (2.0 * t * (1.0 - t) * v2.z) + ((t * t) * v3.z);
         return vec3(x, y, z);
  }


/*--- Dev Diary ---*/

05.04.2020
  I've rediscovered while working on particle position calculation that JavaScript's decimal
  math is broken. Any operation that would suffer from unpredictable rounding errors should
  make use of an external library. For this project, I've decided on decimal.js.

05.04.2020
  Fun fact: even if no operation is performed on a frame buffer object, its values
  will subtly change over time when run through a shader. I spent about four hours pulling
  my hair out today trying to discern why each particle's unaltered wait value became more
  random over the course of each simulation. This is why.

05.08.2020
  After two days and an accidental all-nighter, I've been unable to derive a bug-free
  algorithm that generates final particle positions for each slice given a variable
  particle count per slice. But even if I had figured it out, the approach would have
  had a few drawbacks. For one, it's verbose. My current buggy implementation comes
  out at ~100 lines of obscure calculations. The algorithm also would have restricted
  slices to a rectangular shape, which is artistically inflexible and doesn't match the
  original animation. For these reasons, I've decided to take a more literal approach
  moving forward. Rather than a complex generation system, I'll just write a method that
  returns hard coded position offsets for each slice particle. It's not sophisticated.
  But it is flexible, bug-proof, and can be changed in about 10 minutes with paper and
  pencil.

05.08.2020
  Particle animation is complete! Next up is proper camera movement. In previous projects
  that required smooth position animation, I've spent time deriving trig equations that 
  trace the desired path as closely as possible. It's a relatively quick fix, but results
  in a mathematical mess that's impossible to read or change later. Even if I didn't mind
  the mess, the camera animation in this project is just too complex for the approach to
  be reasonable. So, it's time to learn splines! The three point interpolator I found for
  particle animation is a great start. But, it's hard to imagine having more flexibility
  than a function that takes (locations[], factor) and returns a point in 3D space. Some
  resources I've gathered in early research:
  UCLA Paper: https://www.math.ucla.edu/~baker/149.1.02w/handouts/dd_splines.pdf
  The DeCastleljau Algorithm: https://ibiblio.org/e-notes/Splines/bezier.html

05.15.2020
  To make the process of learning spline interpolation a bit more fun, I ended up building a
  pretty cool litte realtime demo in Java: https://github.com/Xephorium/SplineInterpolator

05.23.2020
  Just realized the slice shape used until now is wider than in the original animation. It
  took all of five minutes to update without pencil or paper. Very grateful for the constant
  offset approach. :) 

05.25.2020
  While stumbling through the refactor to texture each ring block, I've discovered a pretty
  fundamental principle of 3D graphics. It's impossible to draw a vertex with multiple uv
  coordinates. This sounds obvious, but think about a cube. Along the cube's seams, a single
  vertex might have uv coordinates [0,1] for one face and [1,0] for a neighboring face. Yet,
  each vertex processed by the vertex shader must have one and only one set of texture coords.
  The solution to this problem is to duplicate the vertices. Each triangle must be drawn
  between three vertices with data unique to that face (uv coords, normals, etc). The approach
  may sound inefficient, but it's apparently how every rendering engine works under the hood.
  For more info, see the links below:
  https://community.khronos.org/t/texture-coordinates-per-face-index-instead-of-per-vertex/2484
  https://stackoverflow.com/questions/36532924/how-can-i-specify-multiple-uv-coordinates-for-same-vertexes-with-vaos-vbos


/*--- Resources & Factoids ---*/

Resources
  Attributes: https://webglfundamentals.org/webgl/lessons/webgl-attributes.html
  Buffers & Attributes: https://webglfundamentals.org/webgl/lessons/webgl-how-it-works.html
  Buffers vs Framebuffers: https://stackoverflow.com/a/13443183

Factoids
  gl.enableVertexAttribArray(x); // Here, x is the index of the shader attribute to recieve
                                 // vertex info.


/*--- WebGL Basics ---*/

I expect to forget 80% of what I learned in my Computer Graphics class. So here's a
crash course in the core concepts, with a focus on what's needed for this project.

Shaders: WebGL shaders facilitate coding directly to GPU. They're the code blocks at the top
stored as strings and labeled vertex_* or frag_*. These names represent the two halves of each
shader. While the purpose of each half can become muddied in practice, the vertex shader is
generally used to perform operations on vertices and the fragment half determines how the
faces between vertices will appear. (fragment = face in WebGL.) For more details at Stack
Overflow: https://stackoverflow.com/questions/4421261/vertex-shader-vs-fragment-shader

Object display: Say we want to display an object in WebGL. One way to approach this is as
follows. Parse the object data from an .obj file (vertices, UV coordinates, normals, etc.) in
JavaScript. Prepare transformation matrices to be applied to the object (scale, rotate, invert,
etc.) in JavaScript. Send object data and transformation matrices to vertex shader, which
performs transformations on each vertex. Then, in the vertex shader, perform a final transformation
to determine what the object looks like to the virtual camera. Lots of linear algebra and vector
math. Send any required data from the vertex shader to the fragment shader, which determines the
color of each pixel in the resulting image or to display on the screen. Each image we want
to view or process (including the final image rendered to the screen) must be stored in a Frame
Buffer Object (FBO), which we can modify using the GPU through shaders.

Hardware: Anything in a shader (precompiled code stored in a variable) runs on the GPU. Anything
outside a shader (in this case, JavaScript), runs on the CPU. 

Particle Systems: Particle systems in WebGL basically consist of a list of positions, on which
we can perform operations. The positions are then run through a view matrix transformation to
determine their position on the screen, and represented by a single point of variable size and
color.

Particle System Efficiency: Performing operations on each particle position can be done in
JavaScript and then sent to a shader for their final transformation and rendering. However,
this has significant performance limitations. Remember that JavaScript uses the CPU. GPU's
on the other hand were built for massive parallel processing. Therefore, we want to offload
particle calculations to a shader, where they're *much* faster. We do this by storing all 
particle information in pixels of an image. Position image: R = x, G = y, B = z, A = whatever.
Each image can then be stored in a frame buffer object, on which shaders can directly perform
calculations before display. :)