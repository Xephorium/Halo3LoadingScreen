
//////////////////////////////
//// WebGL Halo Animation ////
//////////////////////////////


/*--- Remaining Tasks ---*/

1. Match camera movement
  > Splines!
2. Texture particles
3. Cubes
?. Lines
?. Background


/*--- Animation Timing ---*/

Start Delay: 2,000ms (2s ~ 120 Blender Frames)
Ring Assembly: 66,000ms (66s ~ 3980 Blender Frames)
End Drift: 7,000ms
Loop Length: 75,000ms


/*--- Image Data Organization ---*/

Initial Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Final Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Position
  R: x pos
  G: y pos
  B: z pos
  A: ---

Data Dynamic
  R: Alpha
  G: Brightness
  B: ---
  A: ---

Data Static
  R: Wait
  G: Seed
  B: Ambient
  A: ---


/*--- Frame Buffer Objects ---*/

1. fbo_pos_initial   // Unchanging
2. fbo_pos_final     // Unchanging
3. fbo_pos           // Updated by prog_position
4. fbo_data_dynamic  // Updated by prog_data
5. fbo_data_static   // Unchanging


/*--- Shader Programs ---*/

1. prog_position    // Updates each particle position
2. prog_data        // Updates each particle's dynamic data
3. prog_particle    // Renders particles
4. prog_display     // Displays render to screen


/*--- Cool & Useful Calculations ---*/

  // Procedural Float Generator [-1, 1]
  // Note: Consistently returns the same pseudo-random float for the same two input values.  
  float generate_float(float value_one, float value_two) {
      float seed_one = 78.0;
      float seed_two = 1349.0;
      float magnitude = (mod(floor(value_one * seed_one + value_two * seed_two), 100.0) / 100.0) * 2.0 - 1.0;
      return magnitude;
  }

  // Quadratic Spline Interpolator
  // Note: Returns a position in 3D space representing a particle's location on
  //       a smooth bezier curve between three points given factor t [0-1]. 
  // Source: https://forum.unity.com/threads/getting-a-point-on-a-bezier-curve-given-distance.382785/ 
  vec3 interpolate_location(vec3 v1, vec3 v2, vec3 v3, float t) {
         float x = (((1.0 - t) * (1.0 - t)) * v1.x) + (2.0 * t * (1.0 - t) * v2.x) + ((t * t) * v3.x);
         float y = (((1.0 - t) * (1.0 - t)) * v1.y) + (2.0 * t * (1.0 - t) * v2.y) + ((t * t) * v3.y);
         float z = (((1.0 - t) * (1.0 - t)) * v1.z) + (2.0 * t * (1.0 - t) * v2.z) + ((t * t) * v3.z);
         return vec3(x, y, z);
  }


/*--- Painful Lessons ---*/

> JavaScript's decimal math is broken. Any operation that would suffer from unpredictable
  rounding errors should make use of an external library. In this project, I use decimal.js.

> Even if no operation is performed on a given frame buffer object, its values will
  subtly change over time when returned from a shader. I spent about four hours pulling
  my hair out trying to discern why each particle's unchanging wait value became more
  random over the course of each simulation. This is why.


/*--- Dev Diary ---*/

Slice Particle Positions
  After two days and an accidental all-nighter, I've been unable to derive a bug-free
  algorithm that generates final particle positions for each slice given a variable
  particle count per slice. But even if I had figured it out, the approach would have
  had a few drawbacks. For one, it's verbose. My current imperfect implementation comes
  out at ~100 lines of obscure calculations. The algorithm also would have restricted
  slices to a rectangular shape, which is artistically inflexible and doesn't match the
  original animation. For these reasons, I've decided to take a more literal approach
  moving forward. Rather than a complex generation system, I'd like to just write a
  method that returns hard coded position offsets for each slice particle. It's not
  sophisticated. But it is flexible, bug-proof, and can be changed in about 10 minutes
  with paper and pencil.

Camera Movement Animation
  In previous projects that required smooth position animation, I've spent time deriving
  trig equations that trace the desired path as closely as possible. It's a relatively
  quick fix, but results in a mathematical mess that's impossible to read or change later.
  Even if I didn't mind the mess, the camera animation in this project is just too complex
  for the approach to be reasonable. So, it's time to learn splines! The quadratic spline
  interpolator above is great, but it's hard to imagine having more flexibility than a
  method that takes (locations[], factor) and returns a point in 3D space. Some resources
  I've gathered in early research:

  The DeCastleljau Algorithm: https://ibiblio.org/e-notes/Splines/bezier.html


/*--- Resources & Factoids ---*/

Resources
  Attributes: https://webglfundamentals.org/webgl/lessons/webgl-attributes.html
  Buffers & Attributes: https://webglfundamentals.org/webgl/lessons/webgl-how-it-works.html
  Buffers vs Framebuffers: https://stackoverflow.com/a/13443183

Factoids
  gl.enableVertexAttribArray(x); // Here, x is the index of the shader attribute to recieve
                                 // vertex info.


/*--- WebGL Basics ---*/

I expect to forget 80% of what I learned in my Computer Graphics class. So here's a
crash course in the core concepts, with a focus on what's needed for this project.

Shaders: WebGL shaders facilitate coding directly to GPU. They're the code blocks at the top
stored as strings and labeled vertex_* or frag_*. These names represent the two halves of each
shader. While the purpose of each half can become muddied in practice, the vertex shader is
generally used to perform operations on vertices and the fragment half determines how the
faces between vertices will appear. (fragment = face in WebGL.) For more details at Stack
Overflow: https://stackoverflow.com/questions/4421261/vertex-shader-vs-fragment-shader

Object display: Say we want to display an object in WebGL. One way to approach this is as
follows. Parse the object data from an .obj file (vertices, UV coordinates, normals, etc.) in
JavaScript. Prepare transformation matrices to be applied to the object (scale, rotate, invert,
etc.) in JavaScript. Send object data and transformation matrices to vertex shader, which
performs transformations on each vertex. Then, in the vertex shader, perform a final transformation
to determine what the object looks like to the virtual camera. Lots of linear algebra and vector
math. Send any required data from the vertex shader to the fragment shader, which determines the
color of each pixel in the resulting image or to display on the screen. Each image we want
to view or process (including the final image rendered to the screen) must be stored in a Frame
Buffer Object (FBO), which we can modify using the GPU through shaders.

Hardware: Anything in a shader (precompiled code stored in a variable) runs on the GPU. Anything
outside a shader (in this case, JavaScript), runs on the CPU. 

Particle Systems: Particle systems in WebGL basically consist of a list of positions, on which
we can perform operations. The positions are then run through a view matrix transformation to
determine their position on the screen, and represented by a single point of variable size and
color.

Particle System Efficiency: Performing operations on each particle position can be done in
JavaScript and then sent to a shader for their final transformation and rendering. However,
this has significant performance limitations. Remember that JavaScript uses the CPU. GPU's
on the other hand were built for massive parallel processing. Therefore, we want to offload
particle calculations to a shader, where they're *much* faster. We do this by storing all 
particle information in pixels of an image. Position image: R = x, G = y, B = z, A = whatever.
Each image can then be stored in a frame buffer object, on which shaders can directly perform
calculations before display. :)